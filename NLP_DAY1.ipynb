{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs8xAgApCJ5VqYXW6dqyjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imymemineyay/Studying_NLP/blob/main/NLP_DAY1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **자연어 처리 환경 구성**\n",
        "\n",
        "###1. 텐서플로우 설치\n",
        "\n",
        "\n",
        "```\n",
        "pip install tensorflow # 코랩은 이미 설치됨\n",
        "```\n",
        "\n",
        "### 2. nltk 설치\n",
        "\n",
        "\n",
        "```\n",
        "pip install nltk\n",
        "\n",
        "설치 후\n",
        "\n",
        "import nltk\n",
        "nltk.download()\n",
        "```\n",
        "\n",
        "### 3. konlpy 설치\n",
        "\n",
        "```\n",
        "pip install konlpy\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "43Ay4Xkjr7lr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRO3FTQHN4YP",
        "outputId": "2177a31b-4f0f-4d1e-fa25-27235d6d1c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n"
          ]
        }
      ],
      "source": [
        "pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "\n",
        "konlpy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gJdwJA9usjFI",
        "outputId": "048f8b5c-3f2d-4aa7-a61e-4eee1f489051"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "XgVl-PVisscu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "print('단독입찰보다 복수입찰의 경우 : ', okt.morphs(u'단독입찰보다 복수입찰의 경우'))\n",
        "print('아버지가방에들어가신다 : ', okt.morphs(u'아버지가방에들어가신다'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpCZfHXVs_S5",
        "outputId": "6b7c8f2b-59eb-4e09-d537-efe05dd44d63"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단독입찰보다 복수입찰의 경우 :  ['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
            "아버지가방에들어가신다 :  ['아버지', '가방', '에', '들어가신다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **용어**\n",
        "\n",
        "### what is **단어가방(Bag of Words)**?<br>\n",
        "단어들이 순서와 관계없이 저장된 장소 (== 딕셔너리)<br>\n",
        "\n",
        "### what is **코퍼스(corpus;말뭉치)**?<br>\n",
        "자연어처리(분석)하고자하는 분야와 관련된 단어 집합 <br>\n",
        "ex) 법률 서비스 챗봇 == 법률 코퍼스 <br>\n",
        "법률 코퍼스 : 법률 관련된 용어 (기소, 벌금 등)\n",
        "\n",
        "<hr>\n",
        "\n",
        "### What is **nltk**?<br>\n",
        ": 영어 코퍼스 토큰화 도구\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wQpp7sBjtc0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print('word_tokenize를 사용한 단어 토큰화 결과 : ', word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))\n",
        "\n",
        "print('WordPunctTokenizer를 사용한 단어 토큰화 결과 : ', WordPunctTokenizer().tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbRUWS9FtNXj",
        "outputId": "81e799d5-7741-46d2-ea03-c5c8550a12f6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize를 사용한 단어 토큰화 결과 :  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n",
            "WordPunctTokenizer를 사용한 단어 토큰화 결과 :  ['Tommy', \"'\", 's', 'Don', \"'\", 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\"\n",
        "\n",
        "sent_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIRegcd8vQe-",
        "outputId": "e3e04818-6d44-47a6-de20-0e3078eb9719"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Language is a thing of beauty.',\n",
              " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
              " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
              " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 단위로 태깅해주는 함수\n",
        "from nltk.tag import pos_tag"
      ],
      "metadata": {
        "id": "dShDGb9syCOa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('word_tokenize를 사용한 단어 토큰화 결과 : ', word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))\n",
        "\n",
        "res = word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\")\n",
        "\n",
        "pos_tag(res) # 품사 확인 사이트 (https://happygrammer.github.io/nlp/postag-set/)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxcbVvpgx7kX",
        "outputId": "6ed7d69b-4272-411a-df4f-641526a2d1fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize를 사용한 단어 토큰화 결과 :  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tommy', 'NNP'),\n",
              " (\"'s\", 'POS'),\n",
              " ('Do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('And', 'CC'),\n",
              " ('that', 'IN'),\n",
              " ('’', 'NNP'),\n",
              " ('s', 'VBZ'),\n",
              " ('exactly', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('way', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('our', 'PRP$'),\n",
              " ('machines', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('In', 'IN'),\n",
              " ('order', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('computer', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('understand', 'VB'),\n",
              " ('any', 'DT'),\n",
              " ('text', 'NN'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('need', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('break', 'VB'),\n",
              " ('that', 'IN'),\n",
              " ('word', 'NN'),\n",
              " ('down', 'RP'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('way', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('our', 'PRP$'),\n",
              " ('machine', 'NN'),\n",
              " ('can', 'MD'),\n",
              " ('understand', 'VB'),\n",
              " ('.', '.'),\n",
              " ('That', 'DT'),\n",
              " ('’', 'VBZ'),\n",
              " ('s', 'NN'),\n",
              " ('where', 'WRB'),\n",
              " ('the', 'DT'),\n",
              " ('concept', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('tokenization', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('Natural', 'NNP'),\n",
              " ('Language', 'NNP'),\n",
              " ('Processing', 'NNP'),\n",
              " ('(', '('),\n",
              " ('NLP', 'NNP'),\n",
              " (')', ')'),\n",
              " ('comes', 'VBZ'),\n",
              " ('in', 'IN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOun-LSByYpa",
        "outputId": "de3971d7-2abd-417b-c8a6-897abeb99c68"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-4.5.4.tar.gz (79 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/79.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
            "Building wheels for collected packages: kss, pecab\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-4.5.4-py3-none-any.whl size=54467 sha256=d46a3e812ca8a88e405040bc11fd8fb79fb7e3b5393544caae67839dd4ab7aef\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7b/ba/e620ef5d96a61cdd83bdee4c2bb4aec8a74de5d72fcbb00e80\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646666 sha256=b2f3ae3fb7e5b81d69872212d66eaf7d32f5aa7021427fe3480cf0ea22946557\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "Successfully built kss pecab\n",
            "Installing collected packages: emoji, pecab, kss\n",
            "Successfully installed emoji-1.2.0 kss-4.5.4 pecab-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 문장 토큰화 도구\n",
        "import kss"
      ],
      "metadata": {
        "id": "zdQBjB0OF7jO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"여름입니다. 날씨가 덥습니다. 딥러닝을 공부합니다.\""
      ],
      "metadata": {
        "id": "SXH0FKUMGIsn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kss.split_sentences(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqalZD7tGYCX",
        "outputId": "5aabb339-db77-4f15-ac72-b1c1bee902cf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
            "For your information, Kss also supports mecab backend.\n",
            "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
            "Please refer to following web sites for details:\n",
            "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
            "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['여름입니다.', '날씨가 덥습니다.', '딥러닝을 공부합니다.']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt,Kkma"
      ],
      "metadata": {
        "id": "gjGRx5U9Ge4h"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(\"Okt morphs : \", okt.morphs('NlP를 열심히 공부하고, 취업에 성공합시다.'))\n",
        "print('Okt pos : ', okt.pos('Nlp를 열심히 공부하고, 취업에 성공합시다.'))\n",
        "print('Okt nouns : ', okt.nouns('Nlp를 열심히 공부하고, 취업에 성공합시다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXwnB51TIM4V",
        "outputId": "f55cc70f-a8fb-4b70-cdca-3eb61ed31c4d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okt morphs :  ['NlP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다', '.']\n",
            "Okt pos :  [('Nlp', 'Alpha'), ('를', 'Noun'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), (',', 'Punctuation'), ('취업', 'Noun'), ('에', 'Josa'), ('성공합시다', 'Adjective'), ('.', 'Punctuation')]\n",
            "Okt nouns :  ['를', '공부', '취업']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kkma = Kkma()\n",
        "\n",
        "print(\"Kkma morphs : \", kkma.morphs('NlP를 열심히 공부하고, 취업에 성공합시다.'))\n",
        "print('Kkma pos : ', kkma.pos('Nlp를 열심히 공부하고, 취업에 성공합시다.'))\n",
        "print('Kkma nouns : ', kkma.nouns('Nlp를 열심히 공부하고, 취업에 성공합시다.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2loWTsYIxeK",
        "outputId": "bbb54617-81bf-4195-ce24-aa2cb4353fd3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kkma morphs :  ['NlP', '를', '열심히', '공부', '하', '고', ',', '취업', '에', '성공', '합', '시', '이', '다', '.']\n",
            "Kkma pos :  [('Nlp', 'OL'), ('를', 'JKO'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), (',', 'SP'), ('취업', 'NNG'), ('에', 'JKM'), ('성공', 'NNG'), ('합', 'NNG'), ('시', 'XSN'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF')]\n",
            "Kkma nouns :  ['공부', '취업', '성공', '성공합', '합']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re # 정규표현식\n",
        "\n",
        "text = \"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"\n"
      ],
      "metadata": {
        "id": "2_envh-8Jd6w"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pat = re.compile(r'\\W*\\b\\w{1,2}\\b') # 문자 한 글자 이상 두 글자 이하\n",
        "pat.sub('',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "BXL4luqCK_aA",
        "outputId": "891a8cd5-46e5-4b97-ec72-b35675bf519f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tommy Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlsz2KJqLWGo",
        "outputId": "e6d00405-902c-4062-baeb-bd80b2cfd083"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki9MI7FvMIE6",
        "outputId": "225854aa-b31c-4af9-fdf1-f06a49e28c64"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(stopwords.words('english')))\n",
        "sw = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "_-pb0nYoML-f"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "wt = word_tokenize(text)\n",
        "\n",
        "# 토큰화된 결과 중 stopword에 해당되는 것 제거\n",
        "res = []\n",
        "for w in wt:\n",
        "    if w not in sw:\n",
        "        res.append(w)\n",
        "print(wt)\n",
        "print(res)\n",
        "print(len(res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LJYFJ64NCQ6",
        "outputId": "555b8e5c-8055-4abb-d81f-b1778b54aa70"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n",
            "['Tommy', \"'s\", 'Do', \"n't\", 'And', '’', 'exactly', 'way', 'machines', '.', 'In', 'order', 'get', 'computer', 'understand', 'text', ',', 'need', 'break', 'word', 'way', 'machine', 'understand', '.', 'That', '’', 'concept', 'tokenization', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', '.']\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **nltk stopword list**\n",
        "\n",
        "https://gist.github.com/sebleier/554280\n",
        "\n",
        "\n",
        "#### **한국어 불용어 리스트**\n",
        "https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/ <br>\n",
        "\n",
        "https://github.com/stopwords-iso/stopwords-ko/blob/master/stopwords-ko.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "z7y-GEnbOPmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP를 열심히 공부하고, 취업에 성공합시다.\"\n",
        "sw = \"를 에 고 라고 다\"\n",
        "sw = sw.split(' ')\n",
        "sw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR9MnjpCN_8N",
        "outputId": "3fab76ed-4276-4006-a3e3-6d7435ad6a84"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['를', '에', '고', '라고', '다']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wt = okt.morphs(text)\n",
        "\n",
        "res = [w for w in wt if not w in sw]\n",
        "print('wt : ', wt)\n",
        "print('res : ', res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POoyYfqAO3vs",
        "outputId": "35412ff8-b8a8-4759-ef5a-0f6e2aad6be8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wt :  ['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다', '.']\n",
            "res :  ['NLP', '열심히', '공부', '하고', ',', '취업', '성공합시다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **인코딩**"
      ],
      "metadata": {
        "id": "j4l82WGGT3NT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **원핫인코딩**\n",
        "\n",
        "단어에 대해서 정수로 표현할 때, 원핫인코딩 방식으로 나타낼 수 있음<br>\n",
        "\n",
        "ex) good hi hello → 0 1 2 → 1 0 0 (good) 0 1 0 (hi) 0 0 1 (hello)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PTj-9DNOUXCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Tokenization is a key (and mandatory) aspect of working with text data\n",
        "We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n",
        "Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n",
        "And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n",
        "Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xD7YiuJfPeI8"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw =set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "uR9dKI2kgy6I"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents = sent_tokenize(text)\n",
        "print(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmq4fSSyVIqK",
        "outputId": "af368621-61d4-4c35-c749-c31ff8a318ad"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.', 'But mastering a new language from scratch is quite a daunting prospect.', 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!', 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.', 'And that’s exactly the way with our machines.', 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.', 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.', 'Simply put, we can’t work with text data if we don’t perform tokenization.', 'Yes, it’s really that important!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents\n",
        "len(sents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaJc4L1VPQA",
        "outputId": "92dceec5-36e8-41fe-bfe5-b5da8af3a223"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "pre_sents = []\n",
        "\n",
        "for s in sents:\n",
        "    wt = word_tokenize(s)\n",
        "    res = []\n",
        "    for w in wt:\n",
        "        w = w.lower()\n",
        "        if w not in sw:\n",
        "            if len(w) > 2:\n",
        "                res.append(w)\n",
        "                if w not in vocab:\n",
        "                    vocab[w] = 0\n",
        "                vocab[w] += 1\n",
        "    pre_sents.append(res)\n",
        "print(pre_sents)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvXEMQ6xVfAB",
        "outputId": "d788b0f6-bb32-42c8-c154-131cd8b7fb26"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['tokenization', 'key', 'mandatory', 'aspect', 'working', 'text', 'data', 'discuss', 'various', 'nuances', 'tokenization', 'including', 'handle', 'out-of-vocabulary', 'words', 'oov', 'language', 'thing', 'beauty'], ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'], ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'], ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'], ['exactly', 'way', 'machines'], ['order', 'get', 'computer', 'understand', 'text', 'need', 'break', 'word', 'way', 'machine', 'understand'], ['concept', 'tokenization', 'natural', 'language', 'processing', 'nlp', 'comes'], ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'], ['yes', 'really', 'important']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtzMW96qa1zM",
        "outputId": "7afbafbc-4743-41d0-8183-17a72a5f4873"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('tokenization', 4), ('key', 1), ('mandatory', 1), ('aspect', 1), ('working', 1), ('text', 3), ('data', 2), ('discuss', 1), ('various', 1), ('nuances', 1), ('including', 1), ('handle', 1), ('out-of-vocabulary', 1), ('words', 1), ('oov', 1), ('language', 4), ('thing', 1), ('beauty', 1), ('mastering', 1), ('new', 1), ('scratch', 1), ('quite', 2), ('daunting', 1), ('prospect', 1), ('ever', 1), ('picked', 1), ('mother', 1), ('tongue', 1), ('relate', 1), ('many', 1), ('layers', 1), ('peel', 1), ('syntaxes', 1), ('consider', 1), ('challenge', 1), ('exactly', 1), ('way', 2), ('machines', 1), ('order', 1), ('get', 1), ('computer', 1), ('understand', 2), ('need', 1), ('break', 1), ('word', 1), ('machine', 1), ('concept', 1), ('natural', 1), ('processing', 1), ('nlp', 1), ('comes', 1), ('simply', 1), ('put', 1), ('work', 1), ('perform', 1), ('yes', 1), ('really', 1), ('important', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 높은 순서대로 정렬하여 출력\n",
        "vs = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
        "vs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkxsFgKyY9mM",
        "outputId": "ea6ab55c-00a5-4d7c-f0f1-68215671d361"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tokenization', 4),\n",
              " ('language', 4),\n",
              " ('text', 3),\n",
              " ('data', 2),\n",
              " ('quite', 2),\n",
              " ('way', 2),\n",
              " ('understand', 2),\n",
              " ('key', 1),\n",
              " ('mandatory', 1),\n",
              " ('aspect', 1),\n",
              " ('working', 1),\n",
              " ('discuss', 1),\n",
              " ('various', 1),\n",
              " ('nuances', 1),\n",
              " ('including', 1),\n",
              " ('handle', 1),\n",
              " ('out-of-vocabulary', 1),\n",
              " ('words', 1),\n",
              " ('oov', 1),\n",
              " ('thing', 1),\n",
              " ('beauty', 1),\n",
              " ('mastering', 1),\n",
              " ('new', 1),\n",
              " ('scratch', 1),\n",
              " ('daunting', 1),\n",
              " ('prospect', 1),\n",
              " ('ever', 1),\n",
              " ('picked', 1),\n",
              " ('mother', 1),\n",
              " ('tongue', 1),\n",
              " ('relate', 1),\n",
              " ('many', 1),\n",
              " ('layers', 1),\n",
              " ('peel', 1),\n",
              " ('syntaxes', 1),\n",
              " ('consider', 1),\n",
              " ('challenge', 1),\n",
              " ('exactly', 1),\n",
              " ('machines', 1),\n",
              " ('order', 1),\n",
              " ('get', 1),\n",
              " ('computer', 1),\n",
              " ('need', 1),\n",
              " ('break', 1),\n",
              " ('word', 1),\n",
              " ('machine', 1),\n",
              " ('concept', 1),\n",
              " ('natural', 1),\n",
              " ('processing', 1),\n",
              " ('nlp', 1),\n",
              " ('comes', 1),\n",
              " ('simply', 1),\n",
              " ('put', 1),\n",
              " ('work', 1),\n",
              " ('perform', 1),\n",
              " ('yes', 1),\n",
              " ('really', 1),\n",
              " ('important', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈도수가 2 이상인 단어들에 대해서만 인덱스를 1부터 부여\n",
        "word_index = dict()\n",
        "a = 0\n",
        "for w,f in vs:\n",
        "    if f >=2 :\n",
        "        a += 1\n",
        "        word_index[w] = a\n",
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb8cXGKxZ0Zr",
        "outputId": "a1fac0ef-0bce-4f5d-e152-4fc69f1b4685"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokenization': 1,\n",
              " 'language': 2,\n",
              " 'text': 3,\n",
              " 'data': 4,\n",
              " 'quite': 5,\n",
              " 'way': 6,\n",
              " 'understand': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index['OOV'] = len(word_index)+1 # OOV = Out of Vocabulary"
      ],
      "metadata": {
        "id": "0iSJ5aKXjPhj"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sents = []\n",
        "for s in pre_sents:\n",
        "    enc_sent = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            enc_sent.append(word_index[w])\n",
        "        except KeyError:\n",
        "            enc_sent.append(word_index['OOV'])\n",
        "    enc_sents.append(enc_sent)\n",
        "enc_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z0szyWYcLYs",
        "outputId": "51f800d5-f515-47a1-f961-568c0a3101fe"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 8, 8, 8, 8, 3, 4, 8, 8, 8, 1, 8, 8, 8, 8, 8, 2, 8, 8],\n",
              " [8, 8, 2, 8, 5, 8, 8],\n",
              " [8, 8, 2, 8, 8, 8],\n",
              " [8, 8, 8, 8, 8, 5, 8],\n",
              " [8, 6, 8],\n",
              " [8, 8, 8, 7, 3, 8, 8, 8, 6, 8, 7],\n",
              " [8, 1, 8, 2, 8, 8, 8],\n",
              " [8, 8, 8, 3, 4, 8, 1],\n",
              " [8, 8, 8]]"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **데이터 수집(크롤링) 도구**\n",
        ": selenium, beautifulsoup\n",
        "\n",
        "### **시각화 도구**\n",
        ": matplotlib, plotly, tableau, seaborn,folium\n",
        "\n",
        "### **데이터 분석 도구**\n",
        ": numpy, pandas\n",
        "\n",
        "### **머신러닝도구**\n",
        ": scikit-learn\n",
        "\n",
        "### **딥러닝 프레임워크**\n",
        ": 텐서플로우(케라스), 파이토치, 까페, ..."
      ],
      "metadata": {
        "id": "GkmSwBqaksmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "4aDBOI9jjVQ3"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRisuu6hl5DQ",
        "outputId": "22e850e8-f147-4024-c160-2b9800b3bbe8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['tokenization',\n",
              "  'key',\n",
              "  'mandatory',\n",
              "  'aspect',\n",
              "  'working',\n",
              "  'text',\n",
              "  'data',\n",
              "  'discuss',\n",
              "  'various',\n",
              "  'nuances',\n",
              "  'tokenization',\n",
              "  'including',\n",
              "  'handle',\n",
              "  'out-of-vocabulary',\n",
              "  'words',\n",
              "  'oov',\n",
              "  'language',\n",
              "  'thing',\n",
              "  'beauty'],\n",
              " ['mastering', 'new', 'language', 'scratch', 'quite', 'daunting', 'prospect'],\n",
              " ['ever', 'picked', 'language', 'mother', 'tongue', 'relate'],\n",
              " ['many', 'layers', 'peel', 'syntaxes', 'consider', 'quite', 'challenge'],\n",
              " ['exactly', 'way', 'machines'],\n",
              " ['order',\n",
              "  'get',\n",
              "  'computer',\n",
              "  'understand',\n",
              "  'text',\n",
              "  'need',\n",
              "  'break',\n",
              "  'word',\n",
              "  'way',\n",
              "  'machine',\n",
              "  'understand'],\n",
              " ['concept',\n",
              "  'tokenization',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'nlp',\n",
              "  'comes'],\n",
              " ['simply', 'put', 'work', 'text', 'data', 'perform', 'tokenization'],\n",
              " ['yes', 'really', 'important']]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(pre_sents)"
      ],
      "metadata": {
        "id": "U8zWd7qbmSn9"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9tXtg5bmfua",
        "outputId": "579bcba6-635e-4d0b-c019-88fc5823bb05"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokenization': 1,\n",
              " 'language': 2,\n",
              " 'text': 3,\n",
              " 'data': 4,\n",
              " 'quite': 5,\n",
              " 'way': 6,\n",
              " 'understand': 7,\n",
              " 'key': 8,\n",
              " 'mandatory': 9,\n",
              " 'aspect': 10,\n",
              " 'working': 11,\n",
              " 'discuss': 12,\n",
              " 'various': 13,\n",
              " 'nuances': 14,\n",
              " 'including': 15,\n",
              " 'handle': 16,\n",
              " 'out-of-vocabulary': 17,\n",
              " 'words': 18,\n",
              " 'oov': 19,\n",
              " 'thing': 20,\n",
              " 'beauty': 21,\n",
              " 'mastering': 22,\n",
              " 'new': 23,\n",
              " 'scratch': 24,\n",
              " 'daunting': 25,\n",
              " 'prospect': 26,\n",
              " 'ever': 27,\n",
              " 'picked': 28,\n",
              " 'mother': 29,\n",
              " 'tongue': 30,\n",
              " 'relate': 31,\n",
              " 'many': 32,\n",
              " 'layers': 33,\n",
              " 'peel': 34,\n",
              " 'syntaxes': 35,\n",
              " 'consider': 36,\n",
              " 'challenge': 37,\n",
              " 'exactly': 38,\n",
              " 'machines': 39,\n",
              " 'order': 40,\n",
              " 'get': 41,\n",
              " 'computer': 42,\n",
              " 'need': 43,\n",
              " 'break': 44,\n",
              " 'word': 45,\n",
              " 'machine': 46,\n",
              " 'concept': 47,\n",
              " 'natural': 48,\n",
              " 'processing': 49,\n",
              " 'nlp': 50,\n",
              " 'comes': 51,\n",
              " 'simply': 52,\n",
              " 'put': 53,\n",
              " 'work': 54,\n",
              " 'perform': 55,\n",
              " 'yes': 56,\n",
              " 'really': 57,\n",
              " 'important': 58}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AaaR9wBmyvW",
        "outputId": "4aedf997-3d7b-4194-86dc-6a04e0d6fc45"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('tokenization', 4),\n",
              "             ('key', 1),\n",
              "             ('mandatory', 1),\n",
              "             ('aspect', 1),\n",
              "             ('working', 1),\n",
              "             ('text', 3),\n",
              "             ('data', 2),\n",
              "             ('discuss', 1),\n",
              "             ('various', 1),\n",
              "             ('nuances', 1),\n",
              "             ('including', 1),\n",
              "             ('handle', 1),\n",
              "             ('out-of-vocabulary', 1),\n",
              "             ('words', 1),\n",
              "             ('oov', 1),\n",
              "             ('language', 4),\n",
              "             ('thing', 1),\n",
              "             ('beauty', 1),\n",
              "             ('mastering', 1),\n",
              "             ('new', 1),\n",
              "             ('scratch', 1),\n",
              "             ('quite', 2),\n",
              "             ('daunting', 1),\n",
              "             ('prospect', 1),\n",
              "             ('ever', 1),\n",
              "             ('picked', 1),\n",
              "             ('mother', 1),\n",
              "             ('tongue', 1),\n",
              "             ('relate', 1),\n",
              "             ('many', 1),\n",
              "             ('layers', 1),\n",
              "             ('peel', 1),\n",
              "             ('syntaxes', 1),\n",
              "             ('consider', 1),\n",
              "             ('challenge', 1),\n",
              "             ('exactly', 1),\n",
              "             ('way', 2),\n",
              "             ('machines', 1),\n",
              "             ('order', 1),\n",
              "             ('get', 1),\n",
              "             ('computer', 1),\n",
              "             ('understand', 2),\n",
              "             ('need', 1),\n",
              "             ('break', 1),\n",
              "             ('word', 1),\n",
              "             ('machine', 1),\n",
              "             ('concept', 1),\n",
              "             ('natural', 1),\n",
              "             ('processing', 1),\n",
              "             ('nlp', 1),\n",
              "             ('comes', 1),\n",
              "             ('simply', 1),\n",
              "             ('put', 1),\n",
              "             ('work', 1),\n",
              "             ('perform', 1),\n",
              "             ('yes', 1),\n",
              "             ('really', 1),\n",
              "             ('important', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **패딩 작업**\n",
        "\n",
        "단어(pad), 값(0)으로 설정하여 길이를 맞춤"
      ],
      "metadata": {
        "id": "L4eRgsIJng8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"
      ],
      "metadata": {
        "id": "QnPFps5WnT8a"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **단어 정수 인코딩**"
      ],
      "metadata": {
        "id": "g-TsS0YwoIPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "\n",
        "# 모든 단어에 대해 번호 부여 작업\n",
        "tok.fit_on_texts(pre_sentences)"
      ],
      "metadata": {
        "id": "puFL8-ryoKLI"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 단어에 대해 번호 표시\n",
        "encoded = tok.texts_to_sequences(pre_sentences)\n",
        "encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXp4UcyPoUh4",
        "outputId": "2a8356da-a77c-4334-cd1b-090284e48519"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(i) for i in encoded)"
      ],
      "metadata": {
        "id": "U6rMRGYNpsPB"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoded에 저장된 각각의 리스트 길이 통일화\n",
        "for s in encoded:\n",
        "    while (len(s) < max_len) :\n",
        "        s.append(0)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_7CDTAdom_L",
        "outputId": "0c800d2e-5dc7-4c0c-8e20-d8cc8fb96c7e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3, 0, 0], [2, 5, 3, 0], [2, 4, 3, 0], [6, 1, 0, 0], [1, 7, 4, 1], [4, 1, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.array(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnJAmICFqOjl",
        "outputId": "368569e9-8a0e-4c0b-e3f6-3c79ff928f85"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3, 0, 0],\n",
              "       [2, 5, 3, 0],\n",
              "       [2, 4, 3, 0],\n",
              "       [6, 1, 0, 0],\n",
              "       [1, 7, 4, 1],\n",
              "       [4, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스 도구를 이용한 패딩 작업 常用\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. 인코딩\n",
        "encoded = tok.texts_to_sequences(pre_sentences)\n",
        "print(encoded)\n",
        "\n",
        "# 2. 패딩 (pre가 기본값, maxlen 길이 설정)\n",
        "padded = pad_sequences(encoded, padding = 'post')\n",
        "print([padded])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MinVBPe0qlEG",
        "outputId": "6a399699-54f1-442e-cc23-c1fd9f321e66"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]\n",
            "[array([[2, 3, 0, 0],\n",
            "       [2, 5, 3, 0],\n",
            "       [2, 4, 3, 0],\n",
            "       [6, 1, 0, 0],\n",
            "       [1, 7, 4, 1],\n",
            "       [4, 1, 0, 0]], dtype=int32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 패딩 (pre가 기본값, maxlen 길이 설정)\n",
        "padded = pad_sequences(encoded, padding = 'post', maxlen=2, truncating = 'post')\n",
        "print([padded])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnI1-_TprOfo",
        "outputId": "8243e533-fc94-48e6-8eb1-7a1ed7f7f2f3"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2, 3],\n",
            "       [2, 5],\n",
            "       [2, 4],\n",
            "       [6, 1],\n",
            "       [1, 7],\n",
            "       [4, 1]], dtype=int32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 패딩 (pre가 기본값, maxlen 길이 설정)\n",
        "padded = pad_sequences(encoded, padding = 'post', maxlen=2, truncating = 'pre')\n",
        "print([padded])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXef-s5wsJfU",
        "outputId": "1843489f-94e2-4a55-91f0-67d03d95c53a"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2, 3],\n",
            "       [5, 3],\n",
            "       [4, 3],\n",
            "       [6, 1],\n",
            "       [4, 1],\n",
            "       [4, 1]], dtype=int32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **원핫인코딩**\n",
        "\n",
        "#### 단어의 종류 : 10개 → 인코딩\n",
        "\n",
        "머신러닝/ 딥러닝 언어모델링 (분류/생성/이해...)\n",
        "\n",
        "0~9번까지 번호 부여<br>\n",
        "(0: sky, 1:computer, 2: pencil...)<br>\n",
        "문제점 <br>\n",
        ": 1+2 = 3 (sky+computer = pencil의 관계식 생성)\n",
        "\n",
        "**→ 각각의 단어가 구분이 가능하도록 원핫인코딩 필요**\n",
        "\n",
        "원핫인코딩 표현<br>\n",
        "sky : 100000<br>\n",
        "computer : 010000<br>\n",
        "pencil : 001000<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bc3mCcgjv6j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어의 종류가 10만개 있을 시,\n",
        "\n",
        "원핫인코딩하면 각 단어는 10만차원 벡터 공간에 임베딩되어짐\n",
        "\n",
        "sky : 1000000...0000 (원핫벡터 - 메모리의 낭비 너무 심함) 한 자리 = 4byte  → 단어 하나가 40만 바이트가 됨\n",
        "\n",
        "따라서 이렇게 임베드하지 않음\n",
        "\n",
        "차원 감소 기능(신경망에서 구현함, 이미 생성되어 있는 것)을 이용하여 5차원 벡터공간에 임베딩함\n",
        "[1.7 - 3.5 0.3 0.1 0.2] : dense 벡터\n",
        "\n",
        "일반적으로 밀집벡터 형태로 표현함"
      ],
      "metadata": {
        "id": "Jf6P4giZyWFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = okt.morphs('자연어처리 공부를 합니다.')\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-wwa0-HyTFk",
        "outputId": "0adedc5e-ab40-43f7-d3db-20e241b1baba"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['자연어', '처리', '공부', '를', '합니다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스\n",
        "text = '점심 메뉴로 소고기볶음밥 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.'"
      ],
      "metadata": {
        "id": "ITUQpvTDyg4A"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = Tokenizer()\n",
        "tok.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "L1CkZK3SzVGG"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_index # copus 에 등록된 단어 8개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjfKxlXLzZnB",
        "outputId": "9b3e5329-d855-4fd7-9c80-65ca821ff5fb"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'소고기볶음밥': 1,\n",
              " '점심': 2,\n",
              " '메뉴로': 3,\n",
              " '먹었습니다': 4,\n",
              " '너무': 5,\n",
              " '맛있어요': 6,\n",
              " '또': 7,\n",
              " '먹을래요': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = '내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.'"
      ],
      "metadata": {
        "id": "rNQmTRsOzahc"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tok.texts_to_sequences([test])[0]\n",
        "encoded # 메뉴로, 소고기볶음밥, 또"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rh3Ki2xzi6O",
        "outputId": "fc7df21e-ef2d-4bed-9c03-c6ad483a5b78"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 1, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "o_v = to_categorical(encoded) # 코퍼스에 등록된 단어가 8개이기 때문에 길이가 8이어야함\n",
        "o_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9rntgHKzqKG",
        "outputId": "2819ad71-e820-4e98-93f0-a518f15162dd"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    }
  ]
}